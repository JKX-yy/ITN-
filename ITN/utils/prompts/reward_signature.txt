@torch.jit.script
def compute_reward(fingertip_midpoint_pos: Tensor, nut_pos: Tensor, bolt_tip_pos: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:
    
    # Adjusted temperature parameters for reward transformations
    temp_pick = torch.tensor(0.1)  # Increased to make initial progress more rewarding
    temp_place = torch.tensor(0.4)  # Increase, make this step more rewarding

    # Calculate Distance Based on Reward Components
    distance_finger_to_nut = torch.norm(fingertip_midpoint_pos - nut_pos, p=2, dim=-1)
    distance_nut_to_bolttip = torch.norm(nut_pos - bolt_tip_pos, p=2, dim=-1)

    # Adjusted rewards for each stage of the task with new scaling
    reward_pick = -distance_finger_to_nut 
    reward_place = -distance_nut_to_bolttip

    # Transform rewards with exponential function to normalize and control scale
    reward_pick_exp = torch.exp(reward_pick / temp_pick)
    reward_place_exp = torch.exp(reward_place / temp_place)

    # Combine rewards for total reward, ensuring sequential completion by multiplication
    total_reward = reward_pick_exp  * reward_place_exp

    # Reward components dictionary
    rewards_dict = {
        "reward_pick": reward_pick_exp,
        "reward_place": reward_place_exp,}

    return total_reward, rewards_dict

      